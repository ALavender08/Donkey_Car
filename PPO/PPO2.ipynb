{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebe2c9ef-edf9-40e9-b72b-f49d715b0d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import gym_donkeycar\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow\n",
    "import shimmy\n",
    "from rich.live import Live\n",
    "import stable_baselines3\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "153e811c-cdd9-4bdb-80ab-cfa8be254d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "DONKEY_GYM = True\n",
    "DONKEY_SIM_PATH = \"remote\"\n",
    "DONKEY_GYM_ENV_NAME = \"donkey-generated-track-v0\"\n",
    "body_style = \"car01\"\n",
    "body_rgb = (128, 128, 128)\n",
    "car_name = \"PPO2\"\n",
    "font_size = 100\n",
    "WEB_CONTROL_PORT = int(os.getenv(\"WEB_CONTROL_PORT\", 8887))\n",
    "port = 9091"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b17fbafc-04ed-458f-a24e-b7fe8f051d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/mnt/d/donkeycar_model/PPO2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ae91565-d9fe-4621-b3c2-4b571a2096ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = { \"DONKEY_GYM\":DONKEY_GYM, \"DONKEY_SIM_PATH\":DONKEY_SIM_PATH, \"DONKEY_GYM_ENV_NAME\":DONKEY_GYM_ENV_NAME, \n",
    "         \"body_style\":body_style, \"body_rgb\":body_rgb, \"car_name\":car_name, \"font_size\":font_size, \n",
    "         \"WEB_CONTROL_PORT\" : WEB_CONTROL_PORT, \"port\" : port }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "804ca74f-b75f-4e51-90b3-e4074901877d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym_donkeycar.core.client:connecting to localhost:9091 \n",
      "/home/user/miniconda3/envs/donkey/lib/python3.11/site-packages/gym/spaces/box.py:78: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "INFO:gym_donkeycar.envs.donkey_sim:on need car config\n",
      "INFO:gym_donkeycar.envs.donkey_sim:sending car config.\n",
      "INFO:gym_donkeycar.envs.donkey_sim:sim started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting DonkeyGym env\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"donkey-minimonaco-track-v0\", conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8571d43d-49f5-4726-818f-6c40a48c754f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/donkey/lib/python3.11/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "/home/user/miniconda3/envs/donkey/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:155: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 500`, after every 7 untruncated mini-batches, there will be a truncated mini-batch of size 52\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=500 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.6     |\n",
      "|    ep_rew_mean     | -33.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 35       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 83.3        |\n",
      "|    ep_rew_mean          | -6.25       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 29          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 1000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023145609 |\n",
      "|    clip_fraction        | 0.479       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | -0.00134    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 115         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | 0.0705      |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 599         |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 38.1     |\n",
      "|    ep_rew_mean          | -24.5    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 19       |\n",
      "|    iterations           | 3        |\n",
      "|    time_elapsed         | 76       |\n",
      "|    total_timesteps      | 1500     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 18.05832 |\n",
      "|    clip_fraction        | 0.63     |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -2.84    |\n",
      "|    explained_variance   | 0.671    |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 97.6     |\n",
      "|    n_updates            | 20       |\n",
      "|    policy_gradient_loss | 0.162    |\n",
      "|    std                  | 1        |\n",
      "|    value_loss           | 431      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 32.3     |\n",
      "|    ep_rew_mean          | -46.6    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 17       |\n",
      "|    iterations           | 4        |\n",
      "|    time_elapsed         | 114      |\n",
      "|    total_timesteps      | 2000     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 4.171134 |\n",
      "|    clip_fraction        | 0.917    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -2.87    |\n",
      "|    explained_variance   | -4.01    |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 19.6     |\n",
      "|    n_updates            | 30       |\n",
      "|    policy_gradient_loss | 0.35     |\n",
      "|    std                  | 1.03     |\n",
      "|    value_loss           | 89.5     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 28.3      |\n",
      "|    ep_rew_mean          | -43.1     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 15        |\n",
      "|    iterations           | 5         |\n",
      "|    time_elapsed         | 157       |\n",
      "|    total_timesteps      | 2500      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 10.274952 |\n",
      "|    clip_fraction        | 0.885     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.9      |\n",
      "|    explained_variance   | -0.569    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 17.8      |\n",
      "|    n_updates            | 40        |\n",
      "|    policy_gradient_loss | 0.13      |\n",
      "|    std                  | 1.04      |\n",
      "|    value_loss           | 113       |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/donkey/lib/python3.11/site-packages/stable_baselines3/common/utils.py:573: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
      "  return th.as_tensor(obs, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing 2500 : -103.60287907748804\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 28.2     |\n",
      "|    ep_rew_mean     | -103     |\n",
      "| time/              |          |\n",
      "|    fps             | 15       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 32       |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 24.3       |\n",
      "|    ep_rew_mean          | -76.8      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 13         |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 73         |\n",
      "|    total_timesteps      | 1000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.43433112 |\n",
      "|    clip_fraction        | 0.782      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.93      |\n",
      "|    explained_variance   | 0.326      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 94.3       |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | 0.16       |\n",
      "|    std                  | 1.05       |\n",
      "|    value_loss           | 235        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 23.2        |\n",
      "|    ep_rew_mean          | -70.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 113         |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.075460374 |\n",
      "|    clip_fraction        | 0.504       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.94       |\n",
      "|    explained_variance   | 0.483       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 85.7        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | 0.0531      |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 179         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# training\u001b[39;00m\n\u001b[32m      8\u001b[39m obs = env.reset()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m model.save(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m(i+\u001b[32m1\u001b[39m)*\u001b[32m2500\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pth\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# testing\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/donkey/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[39m, in \u001b[36mPPO.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    303\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[32m    304\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    309\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    310\u001b[39m ) -> SelfPPO:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/donkey/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:324\u001b[39m, in \u001b[36mOnPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_timesteps < total_timesteps:\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     continue_training = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/donkey/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:218\u001b[39m, in \u001b[36mOnPolicyAlgorithm.collect_rollouts\u001b[39m\u001b[34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[39m\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    214\u001b[39m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[32m    215\u001b[39m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[32m    216\u001b[39m         clipped_actions = np.clip(actions, \u001b[38;5;28mself\u001b[39m.action_space.low, \u001b[38;5;28mself\u001b[39m.action_space.high)\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m new_obs, rewards, dones, infos = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[38;5;28mself\u001b[39m.num_timesteps += env.num_envs\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/donkey/lib/python3.11/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:222\u001b[39m, in \u001b[36mVecEnv.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    216\u001b[39m \u001b[33;03mStep the environments with the given action\u001b[39;00m\n\u001b[32m    217\u001b[39m \n\u001b[32m    218\u001b[39m \u001b[33;03m:param actions: the action\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[33;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[38;5;28mself\u001b[39m.step_async(actions)\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/donkey/lib/python3.11/site-packages/stable_baselines3/common/vec_env/vec_transpose.py:97\u001b[39m, in \u001b[36mVecTransposeImage.step_wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> VecEnvStepReturn:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     observations, rewards, dones, infos = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvenv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m     \u001b[38;5;66;03m# Transpose the terminal observations\u001b[39;00m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m idx, done \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dones):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/donkey/lib/python3.11/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:71\u001b[39m, in \u001b[36mDummyVecEnv.step_wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.buf_dones[env_idx]:\n\u001b[32m     69\u001b[39m         \u001b[38;5;66;03m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[32m     70\u001b[39m         \u001b[38;5;28mself\u001b[39m.buf_infos[env_idx][\u001b[33m\"\u001b[39m\u001b[33mterminal_observation\u001b[39m\u001b[33m\"\u001b[39m] = obs\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m         obs, \u001b[38;5;28mself\u001b[39m.reset_infos[env_idx] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m     \u001b[38;5;28mself\u001b[39m._save_obs(env_idx, obs)\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._obs_from_buf(), np.copy(\u001b[38;5;28mself\u001b[39m.buf_rews), np.copy(\u001b[38;5;28mself\u001b[39m.buf_dones), deepcopy(\u001b[38;5;28mself\u001b[39m.buf_infos))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/donkey/lib/python3.11/site-packages/stable_baselines3/common/monitor.py:83\u001b[39m, in \u001b[36mMonitor.reset\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     81\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected you to pass keyword argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m into reset\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     82\u001b[39m     \u001b[38;5;28mself\u001b[39m.current_reset_info[key] = value\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/donkey/lib/python3.11/site-packages/shimmy/openai_gym_compatibility.py:234\u001b[39m, in \u001b[36mGymV21CompatibilityV0.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    230\u001b[39m     warn(\n\u001b[32m    231\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGym v21 environment do not accept options as a reset parameter, options=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    232\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m obs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgym_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode == \u001b[33m\"\u001b[39m\u001b[33mhuman\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    237\u001b[39m     \u001b[38;5;28mself\u001b[39m.render()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/donkey/lib/python3.11/site-packages/gym/wrappers/order_enforcing.py:18\u001b[39m, in \u001b[36mOrderEnforcing.reset\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs):\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mself\u001b[39m._has_reset = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gym-donkeycar/gym_donkeycar/envs/donkey_env.py:143\u001b[39m, in \u001b[36mDonkeyEnv.reset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;28mself\u001b[39m.viewer.handler.send_control(\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1.0\u001b[39m)\n\u001b[32m    142\u001b[39m time.sleep(\u001b[32m0.1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mviewer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;28mself\u001b[39m.viewer.handler.send_control(\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1.0\u001b[39m)\n\u001b[32m    145\u001b[39m time.sleep(\u001b[32m0.1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gym-donkeycar/gym_donkeycar/envs/donkey_sim.py:99\u001b[39m, in \u001b[36mDonkeyUnitySimContoller.reset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gym-donkeycar/gym_donkeycar/envs/donkey_sim.py:404\u001b[39m, in \u001b[36mDonkeyUnitySimHandler.reset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    402\u001b[39m \u001b[38;5;28mself\u001b[39m.send_reset_car()\n\u001b[32m    403\u001b[39m \u001b[38;5;28mself\u001b[39m.timer.reset()\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m time.sleep(\u001b[32m1\u001b[39m)\n\u001b[32m    405\u001b[39m \u001b[38;5;28mself\u001b[39m.image_array = np.zeros(\u001b[38;5;28mself\u001b[39m.camera_img_size)\n\u001b[32m    406\u001b[39m \u001b[38;5;28mself\u001b[39m.image_array_b = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    # preparation\n",
    "    if i == 0:\n",
    "        model = model = PPO(\"CnnPolicy\", env, n_steps=500, verbose=2 )\n",
    "        Log={\"TestReward\":[]}\n",
    "\n",
    "    # training\n",
    "    obs = env.reset()\n",
    "    model.learn(total_timesteps=2500, progress_bar=0)\n",
    "    model.save(f'{folder}{(i+1)*2500}.pth')\n",
    "\n",
    "    # testing\n",
    "    tem = 0\n",
    "    for _ in range(20):\n",
    "        obs = env.reset()\n",
    "        while True:\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            # print (reward, done)\n",
    "            env.render()\n",
    "            tem += reward\n",
    "            if done: break\n",
    "    Log[\"TestReward\"].append(tem/20)\n",
    "    print(f\"testing {(i+1)*2500} : {tem/20}\")\n",
    "    np.save(f\"{folder}/Log_PPO2-{(i+1)*2500}.npy\", Log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e11a21ce-461f-46bc-a1c4-bdde300d4317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1343.4235813868684\n"
     ]
    }
   ],
   "source": [
    "num = 60000\n",
    "test_model = PPO.load(f\"{folder}{num}.pth\")\n",
    "obs = env.reset()\n",
    "tem = 0\n",
    "while True:\n",
    "    action, _states = test_model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    # print (reward, done)\n",
    "    env.render()\n",
    "    tem += reward\n",
    "    if done: break\n",
    "print(tem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99bbb982-83fc-45be-9705-e059ab5ff08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44c2fdb-9342-43eb-9fe9-a9575ba4a3fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
