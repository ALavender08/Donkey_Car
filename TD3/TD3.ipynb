{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebe2c9ef-edf9-40e9-b72b-f49d715b0d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import gym_donkeycar\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow\n",
    "import shimmy\n",
    "from rich.live import Live\n",
    "import stable_baselines3\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "153e811c-cdd9-4bdb-80ab-cfa8be254d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "DONKEY_GYM = True\n",
    "DONKEY_SIM_PATH = \"remote\"\n",
    "DONKEY_GYM_ENV_NAME = \"donkey-generated-track-v0\"\n",
    "body_style = \"car01\"\n",
    "body_rgb = (0, 0, 0)\n",
    "car_name = \"TD3\"\n",
    "font_size = 100\n",
    "WEB_CONTROL_PORT = int(os.getenv(\"WEB_CONTROL_PORT\", 8887))\n",
    "port = 9091"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36313f51-71b1-4da6-9ef6-3ec7def8699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/mnt/d/donkeycar_model/TD3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ae91565-d9fe-4621-b3c2-4b571a2096ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = { \"DONKEY_GYM\":DONKEY_GYM, \"DONKEY_SIM_PATH\":DONKEY_SIM_PATH, \"DONKEY_GYM_ENV_NAME\":DONKEY_GYM_ENV_NAME, \n",
    "         \"body_style\":body_style, \"body_rgb\":body_rgb, \"car_name\":car_name, \"font_size\":font_size, \n",
    "         \"WEB_CONTROL_PORT\" : WEB_CONTROL_PORT, \"port\" : port }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "804ca74f-b75f-4e51-90b3-e4074901877d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym_donkeycar.core.client:connecting to localhost:9091 \n",
      "WARNING:gym_donkeycar.envs.donkey_sim:waiting for sim to start..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting DonkeyGym env\n",
      "Setting default: start_delay 5.0\n",
      "Setting default: max_cte 8.0\n",
      "Setting default: frame_skip 1\n",
      "Setting default: cam_resolution (120, 160, 3)\n",
      "Setting default: log_level 20\n",
      "Setting default: host localhost\n",
      "Setting default: steer_limit 1.0\n",
      "Setting default: throttle_min 0.0\n",
      "Setting default: throttle_max 1.0\n",
      "loading scene mini_monaco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gym_donkeycar.envs.donkey_sim:waiting for sim to start..\n",
      "INFO:gym_donkeycar.envs.donkey_sim:on need car config\n",
      "INFO:gym_donkeycar.envs.donkey_sim:sending car config.\n",
      "INFO:gym_donkeycar.envs.donkey_sim:sim started!\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"donkey-minimonaco-track-v0\", conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a306c901-2385-460e-8049-b4ae62f7f158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "-3.0 False\n",
      "-2.994835054888508 False\n",
      "-2.9959999435906766 False\n",
      "-2.935955752538078 False\n",
      "-2.9132852813647405 False\n",
      "-2.8912285353459692 False\n",
      "-2.8544160772117695 False\n",
      "-2.7459679853766112 False\n",
      "-2.6679570296093726 False\n",
      "-2.6406142432743813 False\n",
      "-2.635615567920262 False\n",
      "-2.5651772521931626 False\n",
      "-2.50328601463232 False\n",
      "-2.425151131907425 False\n",
      "-2.3510559433678524 False\n",
      "-2.312325798124226 False\n",
      "-2.25124055121226 False\n",
      "-2.2205475712153193 False\n",
      "-2.155923386827283 False\n",
      "-2.0445640017176565 False\n",
      "-1.9998280482673438 False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# training\u001b[39;00m\n\u001b[32m      7\u001b[39m obs = model.set_env(env)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m model.save(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m(i+\u001b[32m1\u001b[39m)*\u001b[32m2500\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pth\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# testing\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/donkey/lib/python3.11/site-packages/stable_baselines3/td3/td3.py:227\u001b[39m, in \u001b[36mTD3.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    219\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfTD3,\n\u001b[32m    220\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    225\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    226\u001b[39m ) -> SelfTD3:\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/donkey/lib/python3.11/site-packages/stable_baselines3/common/off_policy_algorithm.py:335\u001b[39m, in \u001b[36mOffPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.train_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_timesteps < total_timesteps:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     rollout = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout.continue_training:\n\u001b[32m    346\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/donkey/lib/python3.11/site-packages/stable_baselines3/common/off_policy_algorithm.py:567\u001b[39m, in \u001b[36mOffPolicyAlgorithm.collect_rollouts\u001b[39m\u001b[34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[39m\n\u001b[32m    564\u001b[39m actions, buffer_actions = \u001b[38;5;28mself\u001b[39m._sample_action(learning_starts, action_noise, env.num_envs)\n\u001b[32m    566\u001b[39m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m new_obs, rewards, dones, infos = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[38;5;28mself\u001b[39m.num_timesteps += env.num_envs\n\u001b[32m    570\u001b[39m num_collected_steps += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/donkey/lib/python3.11/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:222\u001b[39m, in \u001b[36mVecEnv.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    216\u001b[39m \u001b[33;03mStep the environments with the given action\u001b[39;00m\n\u001b[32m    217\u001b[39m \n\u001b[32m    218\u001b[39m \u001b[33;03m:param actions: the action\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[33;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[38;5;28mself\u001b[39m.step_async(actions)\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/donkey/lib/python3.11/site-packages/stable_baselines3/common/vec_env/vec_transpose.py:97\u001b[39m, in \u001b[36mVecTransposeImage.step_wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> VecEnvStepReturn:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     observations, rewards, dones, infos = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvenv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m     \u001b[38;5;66;03m# Transpose the terminal observations\u001b[39;00m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m idx, done \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dones):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/donkey/lib/python3.11/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:59\u001b[39m, in \u001b[36mDummyVecEnv.step_wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> VecEnvStepReturn:\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_envs):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m         obs, \u001b[38;5;28mself\u001b[39m.buf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m.buf_infos[env_idx] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[32m     60\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[32m     63\u001b[39m         \u001b[38;5;28mself\u001b[39m.buf_dones[env_idx] = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/donkey/lib/python3.11/site-packages/stable_baselines3/common/monitor.py:94\u001b[39m, in \u001b[36mMonitor.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.needs_reset:\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTried to step environment that needs reset\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.rewards.append(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/donkey/lib/python3.11/site-packages/shimmy/openai_gym_compatibility.py:250\u001b[39m, in \u001b[36mGymV21CompatibilityV0.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActType) -> \u001b[38;5;28mtuple\u001b[39m[Any, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[32m    242\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment.\u001b[39;00m\n\u001b[32m    243\u001b[39m \n\u001b[32m    244\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    248\u001b[39m \u001b[33;03m        (observation, reward, terminated, truncated, info)\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     obs, reward, done, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgym_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.render_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    253\u001b[39m         \u001b[38;5;28mself\u001b[39m.render()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/donkey/lib/python3.11/site-packages/gym/wrappers/order_enforcing.py:13\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset, \u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling reset()\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     observation, reward, done, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m observation, reward, done, info\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gym-donkeycar/gym_donkeycar/envs/donkey_env.py:136\u001b[39m, in \u001b[36mDonkeyEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.frame_skip):\n\u001b[32m    135\u001b[39m     \u001b[38;5;28mself\u001b[39m.viewer.take_action(action)\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     observation, reward, done, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mviewer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m observation, reward, done, info\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gym-donkeycar/gym_donkeycar/envs/donkey_sim.py:108\u001b[39m, in \u001b[36mDonkeyUnitySimContoller.observe\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mobserve\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Tuple[np.ndarray, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/gym-donkeycar/gym_donkeycar/envs/donkey_sim.py:451\u001b[39m, in \u001b[36mDonkeyUnitySimHandler.observe\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mobserve\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Tuple[np.ndarray, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    450\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_received == \u001b[38;5;28mself\u001b[39m.time_received:\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m         time.sleep(\u001b[32m0.001\u001b[39m)\n\u001b[32m    453\u001b[39m     \u001b[38;5;28mself\u001b[39m.last_received = \u001b[38;5;28mself\u001b[39m.time_received\n\u001b[32m    454\u001b[39m     observation = \u001b[38;5;28mself\u001b[39m.image_array\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    if i == 0:\n",
    "        model = TD3(\"CnnPolicy\", env, n_steps=500, verbose=1, buffer_size=10000 )\n",
    "        Log={\"TestReward\":[]}\n",
    "\n",
    "    # training\n",
    "    obs = model.set_env(env)\n",
    "    model.learn(total_timesteps=2500, progress_bar=0)\n",
    "    model.save(f'{folder}{(i+1)*2500}.pth')\n",
    "\n",
    "    # testing\n",
    "    tem = 0\n",
    "    for _ in range(20):\n",
    "        obs = env.reset()\n",
    "        while True:\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            # print (reward, done)\n",
    "            env.render()\n",
    "            tem += reward\n",
    "            if done: break\n",
    "    Log[\"TestReward\"].append(tem/20)\n",
    "    print(f\"testing {(i+1)*2500} : {tem/20}\")\n",
    "    np.save(f\"{folder}/Log_TD3-{(i+1)*2500}.npy\", Log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5c84fe-ee85-43cc-9d7a-19c2a580aa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 60000\n",
    "test_model = PPO.load(f\"{folder}{num}.pth\")\n",
    "obs = env.reset()\n",
    "tem = 0\n",
    "while True:\n",
    "    action, _states = test_model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    # print (reward, done)\n",
    "    env.render()\n",
    "    tem += reward\n",
    "    if done: break\n",
    "print(tem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99bbb982-83fc-45be-9705-e059ab5ff08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11a21ce-461f-46bc-a1c4-bdde300d4317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
